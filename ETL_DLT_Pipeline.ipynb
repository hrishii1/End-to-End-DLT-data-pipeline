{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5cafb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulbasaur\n",
      "ivysaur\n",
      "venusaur\n",
      "charmander\n",
      "charmeleon\n",
      "charizard\n",
      "squirtle\n",
      "wartortle\n",
      "blastoise\n",
      "caterpie\n"
     ]
    }
   ],
   "source": [
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.auth import BearerTokenAuth\n",
    "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator , JSONLinkPaginator\n",
    "\n",
    "BASE_URL = \"https://pokeapi.co/api/v2/pokemon?limit=10\"\n",
    "\n",
    "client = RESTClient(\n",
    "    base_url= BASE_URL,\n",
    "    paginator=JSONLinkPaginator(next_url_path=\"next\"),\n",
    "    data_selector=\"results\",\n",
    ")\n",
    "\n",
    "count = 0\n",
    "max_count = 10\n",
    "\n",
    "for page in client.paginate():\n",
    "    for pokemon in page:\n",
    "        print(pokemon[\"name\"])\n",
    "        count += 1\n",
    "        if count > max_count:\n",
    "            break \n",
    "    if count >= max_count:\n",
    "        break \n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2de320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulbasaur\n",
      "ivysaur\n",
      "venusaur\n",
      "charmander\n",
      "charmeleon\n",
      "charizard\n",
      "squirtle\n",
      "wartortle\n",
      "blastoise\n",
      "caterpie\n"
     ]
    }
   ],
   "source": [
    "#DLT Resources: \n",
    "import dlt \n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.auth import BearerTokenAuth\n",
    "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator , JSONLinkPaginator\n",
    "\n",
    "BASE_URL = \"https://pokeapi.co/api/v2/pokemon?limit=10\"\n",
    "\n",
    "@dlt.resource()\n",
    "def paginated_gatter():\n",
    "    client = RESTClient(\n",
    "        base_url= BASE_URL,\n",
    "        paginator=JSONLinkPaginator(next_url_path=\"next\"),\n",
    "        data_selector=\"results\",\n",
    "    )\n",
    "\n",
    "    count = 0\n",
    "    max_count = 10\n",
    "\n",
    "    for page in client.paginate():\n",
    "        for pokemon in page:\n",
    "            print(pokemon[\"name\"])\n",
    "            count += 1\n",
    "            if count > max_count:\n",
    "                break \n",
    "        if count >= max_count:\n",
    "            break \n",
    "        yield page \n",
    "for record in paginated_gatter():\n",
    "    print(record)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df2f717d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nimport dlt\\nfrom dlt.sources.helpers.rest_client import RESTClient\\nfrom dlt.sources.helpers.rest_client.auth import BearerTokenAuth\\nfrom dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\\nfrom google.colab import userdata\\n\\nos.environ[\"ACCESS_TOKEN\"] = userdata.get(\\'ACCESS_TOKEN\\') # <--- set ENV in the special format: uppercase secret name\\nevents_data = []\\n\\n\\n@dlt.resource(name=\"events\")\\ndef paginated_getter(\\n    access_token=dlt.secrets.value  # <--- set the secret variable \"access_token\" here\\n):\\n    client = RESTClient(\\n        base_url=\"https://api.github.com/repos/DataTalksClub/data-engineering-zoomcamp/\",\\n        auth=BearerTokenAuth(token=access_token), # <--- use the variable \"access_token\" as usual\\n        paginator=HeaderLinkPaginator(links_next_key=\"next\"),\\n    )\\n\\n    for page in client.paginate(\"events\"):\\n        yield page\\n\\n\\nfor page_data in paginated_getter():\\n    events_data.append(page_data)\\n    print(page_data)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dlt config and secrets: \n",
    "\"\"\"import os\n",
    "import dlt\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.auth import BearerTokenAuth\n",
    "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"ACCESS_TOKEN\"] = userdata.get('ACCESS_TOKEN') # <--- set ENV in the special format: uppercase secret name\n",
    "events_data = []\n",
    "\n",
    "\n",
    "@dlt.resource(name=\"events\")\n",
    "def paginated_getter(\n",
    "    access_token=dlt.secrets.value  # <--- set the secret variable \"access_token\" here\n",
    "):\n",
    "    client = RESTClient(\n",
    "        base_url=\"https://api.github.com/repos/DataTalksClub/data-engineering-zoomcamp/\",\n",
    "        auth=BearerTokenAuth(token=access_token), # <--- use the variable \"access_token\" as usual\n",
    "        paginator=HeaderLinkPaginator(links_next_key=\"next\"),\n",
    "    )\n",
    "\n",
    "    for page in client.paginate(\"events\"):\n",
    "        yield page\n",
    "\n",
    "\n",
    "for page_data in paginated_getter():\n",
    "    events_data.append(page_data)\n",
    "    print(page_data)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814add05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline pokemon_pipeline load step completed in 1.33 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset pokemon\n",
      "The duckdb destination used duckdb:///h:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\pokemon_pipeline.duckdb location to store data\n",
      "Load package 1756006310.096663 is LOADED and contains no failed jobs\n",
      "['trainers', 'trainers__pokemon', 'trainers__pokemon__type']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import dlt\n",
    "import duckdb\n",
    "\n",
    "# Load JSON from local file\n",
    "with open(\"trainers.json\") as f:\n",
    "    data = json.load(f)  # reads the entire JSON array\n",
    "\n",
    "# Define DLT resource\n",
    "@dlt.resource(name=\"trainers\", write_disposition=\"replace\")\n",
    "def load_trainers():\n",
    "    for row in data:\n",
    "        yield row\n",
    "\n",
    "# Define DLT pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"pokemon_pipeline\",\n",
    "    destination=\"duckdb\",  # or \"bigquery\", \"postgres\", etc.\n",
    "    dataset_name=\"pokemon\"\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "info = pipeline.run(load_trainers)\n",
    "print(info)\n",
    "\n",
    "\n",
    "print(pipeline.dataset().schema.data_table_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0868ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id         name  age location__city location__region       _dlt_load_id  \\\n",
      "0   1  Ash Ketchum   10    Pallet Town            Kanto  1756006310.096663   \n",
      "1   2        Misty   12  Cerulean City            Kanto  1756006310.096663   \n",
      "\n",
      "          _dlt_id  \n",
      "0  6TtNoXJuS7kqaQ  \n",
      "1  aX3y4rr0CZpCfA  \n"
     ]
    }
   ],
   "source": [
    "#Shows the tables in the db\n",
    "import duckdb\n",
    "import numpy as np \n",
    "con = duckdb.connect(\"pokemon_pipeline.duckdb\")\n",
    "\n",
    "# Query the actual table name\n",
    "result = con.execute(\"SELECT * FROM pokemon.trainers\").fetchdf()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc526aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.3.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in h:\\data engineering\\projects\\de projects\\etl_dlt_pipeline\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in h:\\data engineering\\projects\\de projects\\etl_dlt_pipeline\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in h:\\data engineering\\projects\\de projects\\etl_dlt_pipeline\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in h:\\data engineering\\projects\\de projects\\etl_dlt_pipeline\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading numpy-2.3.2-cp312-cp312-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.9/12.8 MB 15.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.8/12.8 MB 17.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 21.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 19.1 MB/s  0:00:00\n",
      "Downloading pandas-2.3.2-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 6.3/11.0 MB 29.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/11.0 MB 29.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 25.4 MB/s  0:00:00\n",
      "Installing collected packages: numpy, pandas\n",
      "\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   ---------------------------------------- 2/2 [pandas]\n",
      "\n",
      "Successfully installed numpy-2.3.2 pandas-2.3.2\n"
     ]
    }
   ],
   "source": [
    "#Installs numpy and pandas in the same kernel \n",
    "import sys\n",
    "!\"{sys.executable}\" -m pip install numpy pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        column_name column_type null   key default extra\n",
      "0                id      BIGINT  YES  None    None  None\n",
      "1              name     VARCHAR  YES  None    None  None\n",
      "2               age      BIGINT  YES  None    None  None\n",
      "3    location__city     VARCHAR  YES  None    None  None\n",
      "4  location__region     VARCHAR  YES  None    None  None\n",
      "5      _dlt_load_id     VARCHAR   NO  None    None  None\n",
      "6           _dlt_id     VARCHAR   NO  None    None  None\n"
     ]
    }
   ],
   "source": [
    "#Schema of the table \n",
    "schema = con.execute(\"DESCRIBE pokemon.trainers\").fetchdf()\n",
    "print(schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e9cf0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  pokemon_name  pokemon_type   name age  location.city location.region\n",
      "0      Pikachu      Electric    Ash  10    Pallet Town           Kanto\n",
      "1    Bulbasaur  Grass/Poison    Ash  10    Pallet Town           Kanto\n",
      "2       Staryu         Water  Misty  10  Cerulean City             NaN\n",
      "3      Psyduck           NaN  Misty  10  Cerulean City             NaN\n"
     ]
    }
   ],
   "source": [
    "#Flatten the NESTED JSON:\n",
    "#Sample Nested JSON data:\n",
    "import pandas as pd\n",
    "\n",
    "# Example nested JSON with varying numbers of Pokémon and possibly missing fields\n",
    "data = [\n",
    "    {\n",
    "        \"name\": \"Ash\",\n",
    "        \"age\": 10,\n",
    "        \"location\": {\"city\": \"Pallet Town\", \"region\": \"Kanto\"},\n",
    "        \"pokemon\": [\n",
    "            {\"name\": \"Pikachu\", \"type\": \"Electric\"},\n",
    "            {\"name\": \"Bulbasaur\", \"type\": \"Grass/Poison\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Misty\",\n",
    "        \"age\": 10,\n",
    "        \"location\": {\"city\": \"Cerulean City\"},  # region missing\n",
    "        \"pokemon\": [\n",
    "            {\"name\": \"Staryu\", \"type\": \"Water\"},\n",
    "            {\"name\": \"Psyduck\"}  # type missing\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Brock\",\n",
    "        \"age\": 15,\n",
    "        \"location\": {\"city\": \"Pewter City\", \"region\": \"Kanto\"},\n",
    "        \"pokemon\": []  # no Pokémon\n",
    "    }\n",
    "]\n",
    "\n",
    "# Flatten the JSON\n",
    "df = pd.json_normalize(\n",
    "    data,\n",
    "    record_path=['pokemon'],  # nested list to explode into rows\n",
    "    meta=['name', 'age', ['location', 'city'], ['location', 'region']],\n",
    "    record_prefix=\"pokemon_\",\n",
    "    errors='ignore'\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "894890ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  pokemon_name  pokemon_type trainer_name age           city region\n",
      "0      Pikachu      Electric          Ash  10    Pallet Town  Kanto\n",
      "1    Bulbasaur  Grass/Poison          Ash  10    Pallet Town  Kanto\n",
      "2       Staryu         Water        Misty  10  Cerulean City       \n",
      "3      Psyduck                      Misty  10  Cerulean City       \n"
     ]
    }
   ],
   "source": [
    "#Handling missing values:\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "#Rename columns:\n",
    "df = df.rename(columns={\n",
    "    'name': 'pokemon_name',\n",
    "    'type': 'pokemon_type',\n",
    "    'name': 'trainer_name',\n",
    "    'location.city' : 'city',\n",
    "    'location.region': 'region'\n",
    "})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8ee79",
   "metadata": {},
   "outputs": [
    {
     "ename": "PipelineStepFailed",
     "evalue": "Pipeline execution failed at `step=normalize` when processing package with `load_id=1756094124.197416` with exception:\n\n<class 'dlt.normalize.exceptions.NormalizeJobFailed'>\nJob for `job_id=pokemons.6ba702ac79.typed-jsonl.gz` failed terminally in load with `load_id=1756094124.197416` with message: In schema `pokemon_contract`: In Table: `pokemons` Column: `new_col_test` . Contract on `columns` with `contract_mode=freeze` is violated. Can't add table column `new_col_test` to table `pokemons` because `columns` are frozen. Offending data item: _dlt_id: S26apUUiabzPlQ.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDataValidationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\normalize\\worker.py:253\u001b[39m, in \u001b[36mw_normalize_files\u001b[39m\u001b[34m(config, normalize_storage_config, loader_storage_config, stored_schema, load_id, extracted_items_files)\u001b[39m\n\u001b[32m    249\u001b[39m logger.debug(\n\u001b[32m    250\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing extracted items in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextracted_items_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in load_id\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    251\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with table name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot_table_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and schema \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    252\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m partial_updates = \u001b[43mnormalizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_items_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_table_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m schema_updates.extend(partial_updates)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\normalize\\items_normalizers.py:515\u001b[39m, in \u001b[36mJsonLItemsNormalizer.__call__\u001b[39m\u001b[34m(self, extracted_items_file, root_table_name)\u001b[39m\n\u001b[32m    514\u001b[39m items: List[TDataItem] = json.loadb(line)\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m partial_update = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_normalize_chunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot_table_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmay_have_pua\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_write\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    517\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m schema_updates.append(partial_update)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\normalize\\items_normalizers.py:450\u001b[39m, in \u001b[36mJsonLItemsNormalizer._normalize_chunk\u001b[39m\u001b[34m(self, root_table_name, items, may_have_pua, skip_write)\u001b[39m\n\u001b[32m    442\u001b[39m schema_contract = \u001b[38;5;28mself\u001b[39m._table_contracts.setdefault(\n\u001b[32m    443\u001b[39m     table_name,\n\u001b[32m    444\u001b[39m     schema.resolve_contract_settings_for_table(\n\u001b[32m   (...)\u001b[39m\u001b[32m    448\u001b[39m     ),  \u001b[38;5;66;03m# parent_table, if present, exists in the schema\u001b[39;00m\n\u001b[32m    449\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m partial_table, filters = \u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_schema_contract\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema_contract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_item\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filters:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\common\\schema\\schema.py:335\u001b[39m, in \u001b[36mSchema.apply_schema_contract\u001b[39m\u001b[34m(self, schema_contract, partial_table, data_item, raise_on_freeze)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m raise_on_freeze \u001b[38;5;129;01mand\u001b[39;00m column_mode == \u001b[33m\"\u001b[39m\u001b[33mfreeze\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DataValidationError(\n\u001b[32m    336\u001b[39m         \u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    337\u001b[39m         table_name,\n\u001b[32m    338\u001b[39m         column_name,\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfreeze\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    341\u001b[39m         existing_table,\n\u001b[32m    342\u001b[39m         schema_contract,\n\u001b[32m    343\u001b[39m         data_item,\n\u001b[32m    344\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt add table column `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` to table `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` because\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    345\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `columns` are frozen.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    346\u001b[39m     )\n\u001b[32m    347\u001b[39m \u001b[38;5;66;03m# filter column with name below\u001b[39;00m\n",
      "\u001b[31mDataValidationError\u001b[39m: In schema `pokemon_contract`: In Table: `pokemons` Column: `new_col_test` . Contract on `columns` with `contract_mode=freeze` is violated. Can't add table column `new_col_test` to table `pokemons` because `columns` are frozen. Offending data item: _dlt_id: S26apUUiabzPlQ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNormalizeJobFailed\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\pipeline\\pipeline.py:546\u001b[39m, in \u001b[36mPipeline.normalize\u001b[39m\u001b[34m(self, workers)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m signals.delayed_signals():\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m     \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize_step\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_step_info(normalize_step)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\common\\runners\\pool_runner.py:203\u001b[39m, in \u001b[36mrun_pool\u001b[39m\u001b[34m(config, run_f)\u001b[39m\n\u001b[32m    202\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mRunning pool\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43m_run_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;66;03m# for next run\u001b[39;00m\n\u001b[32m    205\u001b[39m     signals.raise_if_signalled()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\common\\runners\\pool_runner.py:196\u001b[39m, in \u001b[36mrun_pool.<locals>._run_func\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(run_f, Runnable):\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     run_metrics = \u001b[43mrun_f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\normalize\\normalize.py:292\u001b[39m, in \u001b[36mNormalize.run\u001b[39m\u001b[34m(self, pool)\u001b[39m\n\u001b[32m    291\u001b[39m         \u001b[38;5;28mself\u001b[39m._step_info_start_load_id(load_id)\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspool_schema_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# return info on still pending packages (if extractor saved something in the meantime)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\normalize\\normalize.py:246\u001b[39m, in \u001b[36mNormalize.spool_schema_files\u001b[39m\u001b[34m(self, load_id, schema, files)\u001b[39m\n\u001b[32m    245\u001b[39m map_f: TMapFuncType = \u001b[38;5;28mself\u001b[39m.map_parallel \u001b[38;5;28;01mif\u001b[39;00m workers > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.map_single\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspool_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate_normalizers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m load_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\normalize\\normalize.py:186\u001b[39m, in \u001b[36mNormalize.spool_files\u001b[39m\u001b[34m(self, load_id, schema, map_f, files)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mspool_files\u001b[39m(\n\u001b[32m    183\u001b[39m     \u001b[38;5;28mself\u001b[39m, load_id: \u001b[38;5;28mstr\u001b[39m, schema: Schema, map_f: TMapFuncType, files: Sequence[\u001b[38;5;28mstr\u001b[39m]\n\u001b[32m    184\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# process files in parallel or in single thread, depending on map_f\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     schema_updates, writer_metrics = \u001b[43mmap_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# compute metrics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\normalize\\normalize.py:147\u001b[39m, in \u001b[36mNormalize.map_single\u001b[39m\u001b[34m(self, schema, load_id, files)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema: Schema, load_id: \u001b[38;5;28mstr\u001b[39m, files: Sequence[\u001b[38;5;28mstr\u001b[39m]) -> TWorkerRV:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mw_normalize_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalize_storage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_storage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m     validate_and_update_schema(schema, result.schema_updates)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\normalize\\worker.py:259\u001b[39m, in \u001b[36mw_normalize_files\u001b[39m\u001b[34m(config, normalize_storage_config, loader_storage_config, stored_schema, load_id, extracted_items_files)\u001b[39m\n\u001b[32m    258\u001b[39m     writer_metrics = _gather_metrics_and_close(parsed_file_name, in_exception=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NormalizeJobFailed(load_id, job_id, \u001b[38;5;28mstr\u001b[39m(exc), writer_metrics) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mNormalizeJobFailed\u001b[39m: Job for `job_id=pokemons.6ba702ac79.typed-jsonl.gz` failed terminally in load with `load_id=1756094124.197416` with message: In schema `pokemon_contract`: In Table: `pokemons` Column: `new_col_test` . Contract on `columns` with `contract_mode=freeze` is violated. Can't add table column `new_col_test` to table `pokemons` because `columns` are frozen. Offending data item: _dlt_id: S26apUUiabzPlQ.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mPipelineStepFailed\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     24\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     26\u001b[39m pipeline = dlt.pipeline(\n\u001b[32m     27\u001b[39m     pipeline_name=\u001b[33m\"\u001b[39m\u001b[33mpokemon_contract\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m     destination=\u001b[33m\"\u001b[39m\u001b[33mduckdb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m     dataset_name=\u001b[33m\"\u001b[39m\u001b[33mpokemon_dataset\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m info = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaginated_gatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43madd_new_col\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpokemons\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_disposition\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mappend\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\pipeline\\pipeline.py:231\u001b[39m, in \u001b[36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[32m    229\u001b[39m         trace_step = start_trace_step(trace, cast(TPipelineStep, f.\u001b[34m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     step_info = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\pipeline\\pipeline.py:280\u001b[39m, in \u001b[36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m    274\u001b[39m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(\n\u001b[32m    276\u001b[39m         ConfigSectionContext(\n\u001b[32m    277\u001b[39m             pipeline_name=\u001b[38;5;28mself\u001b[39m.pipeline_name, sections=sections, merge_style=merge_func\n\u001b[32m    278\u001b[39m         )\n\u001b[32m    279\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\pipeline\\pipeline.py:743\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, data, destination, staging, dataset_name, credentials, table_name, write_disposition, columns, primary_key, schema, loader_file_format, table_format, schema_contract, refresh)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m.extract(\n\u001b[32m    732\u001b[39m         data,\n\u001b[32m    733\u001b[39m         table_name=table_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    741\u001b[39m         loader_file_format=loader_file_format,\n\u001b[32m    742\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m743\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    744\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.load(destination, dataset_name, credentials=credentials)\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\pipeline\\pipeline.py:231\u001b[39m, in \u001b[36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[32m    229\u001b[39m         trace_step = start_trace_step(trace, cast(TPipelineStep, f.\u001b[34m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     step_info = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\pipeline\\pipeline.py:185\u001b[39m, in \u001b[36mwith_schemas_sync.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    183\u001b[39m     \u001b[38;5;28mself\u001b[39m._schema_storage.commit_live_schema(name)\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     rv = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# because we committed live schema before calling f, we may safely\u001b[39;00m\n\u001b[32m    188\u001b[39m     \u001b[38;5;66;03m# drop all changes in live schemas\u001b[39;00m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._schema_storage.live_schemas.keys()):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\pipeline\\pipeline.py:280\u001b[39m, in \u001b[36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m    274\u001b[39m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(\n\u001b[32m    276\u001b[39m         ConfigSectionContext(\n\u001b[32m    277\u001b[39m             pipeline_name=\u001b[38;5;28mself\u001b[39m.pipeline_name, sections=sections, merge_style=merge_func\n\u001b[32m    278\u001b[39m         )\n\u001b[32m    279\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\.venv\\Lib\\site-packages\\dlt\\pipeline\\pipeline.py:550\u001b[39m, in \u001b[36mPipeline.normalize\u001b[39m\u001b[34m(self, workers)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m n_ex:\n\u001b[32m    549\u001b[39m     step_info = \u001b[38;5;28mself\u001b[39m._get_step_info(normalize_step)\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineStepFailed(\n\u001b[32m    551\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    552\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnormalize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    553\u001b[39m         normalize_step.current_load_id,\n\u001b[32m    554\u001b[39m         n_ex,\n\u001b[32m    555\u001b[39m         step_info,\n\u001b[32m    556\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mn_ex\u001b[39;00m\n",
      "\u001b[31mPipelineStepFailed\u001b[39m: Pipeline execution failed at `step=normalize` when processing package with `load_id=1756094124.197416` with exception:\n\n<class 'dlt.normalize.exceptions.NormalizeJobFailed'>\nJob for `job_id=pokemons.6ba702ac79.typed-jsonl.gz` failed terminally in load with `load_id=1756094124.197416` with message: In schema `pokemon_contract`: In Table: `pokemons` Column: `new_col_test` . Contract on `columns` with `contract_mode=freeze` is violated. Can't add table column `new_col_test` to table `pokemons` because `columns` are frozen. Offending data item: _dlt_id: S26apUUiabzPlQ."
     ]
    }
   ],
   "source": [
    "#Setting up of DATA CONTRACT: \n",
    "import dlt\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.paginators import JSONLinkPaginator\n",
    "\n",
    "BASE_URL = \"https://pokeapi.co/api/v2/pokemon?limit=10\"\n",
    "\n",
    "@dlt.resource(schema_contract={\"tables\": \"evolve\", \"columns\": \"freeze\", \"data_type\": \"evolve\"})\n",
    "def paginated_gatter(add_new_col: bool = False, max_count: int = 10):\n",
    "    client = RESTClient(\n",
    "        base_url=BASE_URL,\n",
    "        paginator=JSONLinkPaginator(next_url_path=\"next\"),\n",
    "        data_selector=\"results\",\n",
    "    )\n",
    "\n",
    "    count = 0\n",
    "    for page in client.paginate():\n",
    "        for pokemon in page:\n",
    "            if add_new_col:\n",
    "                # 🚨 Add new unexpected column\n",
    "                pokemon[\"new_col_test\"] = \"unexpected_value\"\n",
    "            yield pokemon\n",
    "            count += 1\n",
    "            if count >= max_count:\n",
    "                return\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"pokemon_contract\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"pokemon_dataset\"\n",
    ")\n",
    "\n",
    "info = pipeline.run(paginated_gatter(add_new_col=True), table_name=\"pokemons\", write_disposition=\"append\")\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76842b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         name                                   url        _dlt_load_id  \\\n",
      "0   bulbasaur  https://pokeapi.co/api/v2/pokemon/1/  1758695014.0233572   \n",
      "1     ivysaur  https://pokeapi.co/api/v2/pokemon/2/  1758695014.0233572   \n",
      "2    venusaur  https://pokeapi.co/api/v2/pokemon/3/  1758695014.0233572   \n",
      "3  charmander  https://pokeapi.co/api/v2/pokemon/4/  1758695014.0233572   \n",
      "4  charmeleon  https://pokeapi.co/api/v2/pokemon/5/  1758695014.0233572   \n",
      "\n",
      "          _dlt_id  \n",
      "0  jGi+orkj64HXyg  \n",
      "1  QNCDxlXRZtBCFg  \n",
      "2  F07XC/WHc+o1yQ  \n",
      "3  EmdEtmDH7KpqLw  \n",
      "4  WKN9T0AkWNll+Q  \n"
     ]
    }
   ],
   "source": [
    "#Loading with dlt: Using default  method \n",
    "import dlt \n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.auth import BearerTokenAuth\n",
    "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator , JSONLinkPaginator\n",
    "\n",
    "BASE_URL = \"https://pokeapi.co/api/v2/pokemon?limit=10\"\n",
    "\n",
    "@dlt.resource(name = \"pokemon\" ,write_disposition=\"replace\")\n",
    "def paginated_gatter():\n",
    "    client = RESTClient(\n",
    "        base_url= BASE_URL,\n",
    "        paginator=JSONLinkPaginator(next_url_path=\"next\"),\n",
    "        data_selector=\"results\",\n",
    "    )\n",
    "\n",
    "\n",
    "    for page in client.paginate():\n",
    "        for pokemon in page:\n",
    "            yield pokemon\n",
    "    \n",
    "\n",
    "#Define pipeline:\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"Pokemon_names\", \n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"pokemon_dataset\"\n",
    ")\n",
    "\n",
    "#Run the pipeline:\n",
    "load_info = pipeline.run(paginated_gatter())  # <-- No need to specify write_disposition here\n",
    "\n",
    "df = pipeline.dataset().pokemon.df()\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1074cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Pokemon_names load step completed in 0.42 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset pokemon_dataset\n",
      "The duckdb destination used duckdb:///h:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\Pokemon_names.duckdb location to store data\n",
      "Load package 1758695014.0233572 is LOADED and contains no failed jobs\n",
      "['1758695014.0233572']\n",
      "{'pipeline': {'pipeline_name': 'Pokemon_names'}, 'destination_type': 'dlt.destinations.duckdb', 'destination_displayable_credentials': 'duckdb:///h:\\\\Data Engineering\\\\Projects\\\\DE Projects\\\\ETL_DLT_pipeline\\\\Pokemon_names.duckdb', 'destination_name': 'duckdb', 'environment': None, 'staging_type': None, 'staging_name': None, 'staging_displayable_credentials': None, 'destination_fingerprint': '', 'dataset_name': 'pokemon_dataset', 'loads_ids': ['1758695014.0233572'], 'load_packages': [{'load_id': '1758695014.0233572', 'package_path': 'C:\\\\Users\\\\hrish\\\\.dlt\\\\pipelines\\\\Pokemon_names\\\\load\\\\loaded\\\\1758695014.0233572', 'state': 'loaded', 'completed_at': DateTime(2025, 9, 24, 6, 24, 27, 706105, tzinfo=Timezone('UTC')), 'jobs': [{'state': 'completed_jobs', 'file_path': 'C:\\\\Users\\\\hrish\\\\.dlt\\\\pipelines\\\\Pokemon_names\\\\load\\\\loaded\\\\1758695014.0233572\\\\completed_jobs\\\\pokemon.5191bc3dfd.0.insert_values.gz', 'file_size': 28508, 'created_at': DateTime(2025, 9, 24, 6, 24, 27, 227318, tzinfo=Timezone('UTC')), 'elapsed': 0.4787867069244385, 'failed_message': None, 'table_name': 'pokemon', 'file_id': '5191bc3dfd', 'retry_count': 0, 'file_format': 'insert_values', 'is_compressed': True, 'job_id': 'pokemon.5191bc3dfd.insert_values.gz'}], 'schema_hash': 'z0FyA2Blj7n3QasyPR2YnymfYSY36hsWhot7a54fqKQ=', 'schema_name': 'pokemon_names', 'tables': []}], 'first_run': False, 'started_at': DateTime(2025, 9, 24, 6, 24, 27, 318896, tzinfo=Timezone('UTC')), 'finished_at': DateTime(2025, 9, 24, 6, 24, 27, 740803, tzinfo=Timezone('UTC')), 'job_metrics': [{'load_id': '1758695014.0233572', 'job_id': 'pokemon.5191bc3dfd.insert_values.gz', 'file_path': 'C:\\\\Users\\\\hrish\\\\.dlt\\\\pipelines\\\\Pokemon_names\\\\load\\\\normalized\\\\1758695014.0233572\\\\started_jobs\\\\pokemon.5191bc3dfd.0.insert_values.gz', 'table_name': 'pokemon', 'started_at': DateTime(2025, 9, 24, 6, 24, 27, 451195, tzinfo=Timezone('UTC')), 'finished_at': DateTime(2025, 9, 24, 6, 24, 27, 573566, tzinfo=Timezone('UTC')), 'state': 'completed', 'remote_url': None}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pokemon']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gives full TRACE of the pipeline: \n",
    "print(load_info)\n",
    "print(load_info.loads_ids) \n",
    "print(load_info.asdict())\n",
    "pipeline.dataset().schema.data_table_names()  #Table created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f179e80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "  Using cached mysql_connector_python-9.4.0-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
      "Using cached mysql_connector_python-9.4.0-cp312-cp312-win_amd64.whl (16.4 MB)\n",
      "Installing collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-9.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mysql-connector-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf43e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dlt\n",
    "import mysql.connector\n",
    "\n",
    "@dlt.source\n",
    "def mysql_source():\n",
    "    def fetch():\n",
    "        conn = mysql.connector.connect(\n",
    "            host=\"127.0.0.1\",\n",
    "            user=\"root\",\n",
    "            password=\"Password@11\",\n",
    "            database=\"school\"\n",
    "        )\n",
    "        cursor = conn.cursor(dictionary=True)\n",
    "        cursor.execute(\"SELECT * FROM student\")  # Your MySQL table\n",
    "        rows = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        for row in rows:\n",
    "            yield row\n",
    "\n",
    "    # Resource name becomes the DuckDB table name\n",
    "    return dlt.resource(fetch(), name=\"school\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4772162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 21:48:54,284|[WARNING]|10216|9096|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853' to 'my_sql_pipelines_dataset_20250925044853 which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,284|[WARNING]|10216|9096|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853_staging' to 'my_sql_pipelines_dataset_20250925044853_staging which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,315|[WARNING]|10216|9096|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853' to 'my_sql_pipelines_dataset_20250925044853 which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,315|[WARNING]|10216|9096|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853_staging' to 'my_sql_pipelines_dataset_20250925044853_staging which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,541|[WARNING]|10216|9096|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853' to 'my_sql_pipelines_dataset_20250925044853 which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,542|[WARNING]|10216|9096|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853_staging' to 'my_sql_pipelines_dataset_20250925044853_staging which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,547|[WARNING]|10216|8560|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853' to 'my_sql_pipelines_dataset_20250925044853 which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,553|[WARNING]|10216|9096|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853' to 'my_sql_pipelines_dataset_20250925044853 which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,555|[WARNING]|10216|8560|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853_staging' to 'my_sql_pipelines_dataset_20250925044853_staging which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,557|[WARNING]|10216|9096|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853_staging' to 'my_sql_pipelines_dataset_20250925044853_staging which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,567|[WARNING]|10216|3884|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853' to 'my_sql_pipelines_dataset_20250925044853 which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,570|[WARNING]|10216|3884|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853_staging' to 'my_sql_pipelines_dataset_20250925044853_staging which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,634|[WARNING]|10216|9096|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853' to 'my_sql_pipelines_dataset_20250925044853 which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,634|[WARNING]|10216|9096|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853_staging' to 'my_sql_pipelines_dataset_20250925044853_staging which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,642|[WARNING]|10216|9096|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853' to 'my_sql_pipelines_dataset_20250925044853 which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,644|[WARNING]|10216|9096|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853_staging' to 'my_sql_pipelines_dataset_20250925044853_staging which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,653|[WARNING]|10216|9096|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853' to 'my_sql_pipelines_dataset_20250925044853 which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,655|[WARNING]|10216|9096|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853_staging' to 'my_sql_pipelines_dataset_20250925044853_staging which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n",
      "2025-09-24 21:48:54,774|[WARNING]|10216|9096|dlt|client.py|_normalize_identifier:245|Due to normalization dataset name got changed from 'MySql_pipelines_dataset_20250925044853' to 'my_sql_pipelines_dataset_20250925044853 which will be used to create db schemas or folders. `dataset_name` field in the pipeline instance will not be changed. We suggest that you use dataset names that do not need to be normalized or disable dataset name normalization via `enable_dataset_name_normalization` on destination configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline MySql_pipelines load step completed in 0.46 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset my_sql_pipelines_dataset_20250925044853\n",
      "The duckdb destination used duckdb:///h:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\MySql_pipelines.duckdb location to store data\n",
      "Load package 1758775733.7837393 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Define pipeline:\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"MySql_pipelines\", \n",
    "    destination=\"duckdb\", \n",
    "    dev_mode=True,\n",
    ")\n",
    "\n",
    "# Collect the data from the MySQL source\n",
    "data = mysql_source()\n",
    "info = pipeline.run(data)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bc9995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slowly changing dimensions: (SCD 2 example)\n",
    "data = [\n",
    "    {\"name\": \"Vincent Crabbe\", \"designation\": \"student\", \"date_started\": \"1991-09-01T09:00:00Z\"},\n",
    "    {\"name\": \"Gregory Goyle\", \"designation\": \"student\", \"date_started\": \"1991-09-01T09:00:00Z\"},\n",
    "    {\"name\": \"Draco Malfoy\", \"designation\": \"student\", \"date_started\": \"1991-09-01T09:00:00Z\"}\n",
    "]\n",
    "\n",
    "data_updated = [\n",
    "    {\"name\": \"Vincent Crabbe\", \"designation\": \"student\", \"date_started\": \"1991-09-01T09:00:00Z\"},\n",
    "    {\"name\": \"Gregory Goyle\", \"designation\": \"student\", \"date_started\": \"1991-09-01T09:00:00Z\"},\n",
    "    {\"name\": \"Draco Malfoy\", \"designation\": \"expelled\", \"date_started\": \"1991-09-01T09:00:00Z\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a45dfa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline hogwarts_pipeline load step completed in 0.44 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset hogwarts\n",
      "The duckdb destination used duckdb:///h:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\hogwarts_pipeline.duckdb location to store data\n",
      "Load package 1758784144.384754 is LOADED and contains no failed jobs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_dlt_valid_from</th>\n",
       "      <th>_dlt_valid_to</th>\n",
       "      <th>name</th>\n",
       "      <th>designation</th>\n",
       "      <th>date_started</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-25 07:05:37.934117+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Vincent Crabbe</td>\n",
       "      <td>student</td>\n",
       "      <td>1991-09-01 09:00:00+00:00</td>\n",
       "      <td>1758783937.9341166</td>\n",
       "      <td>P94fUt8jUkzx6A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-25 07:05:37.934117+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Gregory Goyle</td>\n",
       "      <td>student</td>\n",
       "      <td>1991-09-01 09:00:00+00:00</td>\n",
       "      <td>1758783937.9341166</td>\n",
       "      <td>Wv4kISjkizZwkQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-25 07:05:37.934117+00:00</td>\n",
       "      <td>2025-09-25 07:06:00.934666+00:00</td>\n",
       "      <td>Draco Malfoy</td>\n",
       "      <td>student</td>\n",
       "      <td>1991-09-01 09:00:00+00:00</td>\n",
       "      <td>1758783937.9341166</td>\n",
       "      <td>r7u0vNojh+gvEQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-25 07:06:00.934666+00:00</td>\n",
       "      <td>2025-09-25 07:08:46.801204+00:00</td>\n",
       "      <td>Draco Malfoy</td>\n",
       "      <td>expelled</td>\n",
       "      <td>1991-09-01 09:00:00+00:00</td>\n",
       "      <td>1758783960.934666</td>\n",
       "      <td>KCWs+o6jOW/aLQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-25 07:08:46.801204+00:00</td>\n",
       "      <td>2025-09-25 07:09:04.384754+00:00</td>\n",
       "      <td>Maximo Malfoy</td>\n",
       "      <td>expelled</td>\n",
       "      <td>1991-09-01 09:00:00+00:00</td>\n",
       "      <td>1758784126.801204</td>\n",
       "      <td>HcX8XVCthXfi5Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-09-25 07:09:04.384754+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Draco Malfoy</td>\n",
       "      <td>student</td>\n",
       "      <td>1991-09-01 09:00:00+00:00</td>\n",
       "      <td>1758784144.384754</td>\n",
       "      <td>r7u0vNojh+gvEQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   _dlt_valid_from                    _dlt_valid_to  \\\n",
       "0 2025-09-25 07:05:37.934117+00:00                              NaT   \n",
       "1 2025-09-25 07:05:37.934117+00:00                              NaT   \n",
       "2 2025-09-25 07:05:37.934117+00:00 2025-09-25 07:06:00.934666+00:00   \n",
       "3 2025-09-25 07:06:00.934666+00:00 2025-09-25 07:08:46.801204+00:00   \n",
       "4 2025-09-25 07:08:46.801204+00:00 2025-09-25 07:09:04.384754+00:00   \n",
       "5 2025-09-25 07:09:04.384754+00:00                              NaT   \n",
       "\n",
       "             name designation              date_started        _dlt_load_id  \\\n",
       "0  Vincent Crabbe     student 1991-09-01 09:00:00+00:00  1758783937.9341166   \n",
       "1   Gregory Goyle     student 1991-09-01 09:00:00+00:00  1758783937.9341166   \n",
       "2    Draco Malfoy     student 1991-09-01 09:00:00+00:00  1758783937.9341166   \n",
       "3    Draco Malfoy    expelled 1991-09-01 09:00:00+00:00   1758783960.934666   \n",
       "4   Maximo Malfoy    expelled 1991-09-01 09:00:00+00:00   1758784126.801204   \n",
       "5    Draco Malfoy     student 1991-09-01 09:00:00+00:00   1758784144.384754   \n",
       "\n",
       "          _dlt_id  \n",
       "0  P94fUt8jUkzx6A  \n",
       "1  Wv4kISjkizZwkQ  \n",
       "2  r7u0vNojh+gvEQ  \n",
       "3  KCWs+o6jOW/aLQ  \n",
       "4  HcX8XVCthXfi5Q  \n",
       "5  r7u0vNojh+gvEQ  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run pipeline with initial dataset:\n",
    "import dlt\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"hogwarts_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"hogwarts\",\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(\n",
    "    data,\n",
    "    table_name=\"creatures\",\n",
    "    write_disposition={\n",
    "        \"disposition\": \"merge\", # <--- specifies that existing data should be merged rather than replaced\n",
    "        \"strategy\": \"scd2\" # <--- enables SCD2 tracking, which keeps historical records of changes\n",
    "    }\n",
    ")\n",
    "print(load_info)\n",
    "\n",
    "pipeline.dataset().creatures.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11adb0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline hogwarts_pipeline load step completed in 0.39 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset hogwarts\n",
      "The duckdb destination used duckdb:///h:\\Data Engineering\\Projects\\DE Projects\\ETL_DLT_pipeline\\hogwarts_pipeline.duckdb location to store data\n",
      "Load package 1758784151.6306615 is LOADED and contains no failed jobs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_dlt_valid_from</th>\n",
       "      <th>_dlt_valid_to</th>\n",
       "      <th>name</th>\n",
       "      <th>designation</th>\n",
       "      <th>date_started</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-25 07:05:37.934117+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Vincent Crabbe</td>\n",
       "      <td>student</td>\n",
       "      <td>1991-09-01 09:00:00+00:00</td>\n",
       "      <td>1758783937.9341166</td>\n",
       "      <td>P94fUt8jUkzx6A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-25 07:05:37.934117+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Gregory Goyle</td>\n",
       "      <td>student</td>\n",
       "      <td>1991-09-01 09:00:00+00:00</td>\n",
       "      <td>1758783937.9341166</td>\n",
       "      <td>Wv4kISjkizZwkQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-25 07:05:37.934117+00:00</td>\n",
       "      <td>2025-09-25 07:06:00.934666+00:00</td>\n",
       "      <td>Draco Malfoy</td>\n",
       "      <td>student</td>\n",
       "      <td>1991-09-01 09:00:00+00:00</td>\n",
       "      <td>1758783937.9341166</td>\n",
       "      <td>r7u0vNojh+gvEQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-25 07:06:00.934666+00:00</td>\n",
       "      <td>2025-09-25 07:08:46.801204+00:00</td>\n",
       "      <td>Draco Malfoy</td>\n",
       "      <td>expelled</td>\n",
       "      <td>1991-09-01 09:00:00+00:00</td>\n",
       "      <td>1758783960.934666</td>\n",
       "      <td>KCWs+o6jOW/aLQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-25 07:08:46.801204+00:00</td>\n",
       "      <td>2025-09-25 07:09:04.384754+00:00</td>\n",
       "      <td>Maximo Malfoy</td>\n",
       "      <td>expelled</td>\n",
       "      <td>1991-09-01 09:00:00+00:00</td>\n",
       "      <td>1758784126.801204</td>\n",
       "      <td>HcX8XVCthXfi5Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-09-25 07:09:04.384754+00:00</td>\n",
       "      <td>2025-09-25 07:09:11.630661+00:00</td>\n",
       "      <td>Draco Malfoy</td>\n",
       "      <td>student</td>\n",
       "      <td>1991-09-01 09:00:00+00:00</td>\n",
       "      <td>1758784144.384754</td>\n",
       "      <td>r7u0vNojh+gvEQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-09-25 07:09:11.630661+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Draco Malfoy</td>\n",
       "      <td>expelled</td>\n",
       "      <td>1991-09-01 09:00:00+00:00</td>\n",
       "      <td>1758784151.6306615</td>\n",
       "      <td>KCWs+o6jOW/aLQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   _dlt_valid_from                    _dlt_valid_to  \\\n",
       "0 2025-09-25 07:05:37.934117+00:00                              NaT   \n",
       "1 2025-09-25 07:05:37.934117+00:00                              NaT   \n",
       "2 2025-09-25 07:05:37.934117+00:00 2025-09-25 07:06:00.934666+00:00   \n",
       "3 2025-09-25 07:06:00.934666+00:00 2025-09-25 07:08:46.801204+00:00   \n",
       "4 2025-09-25 07:08:46.801204+00:00 2025-09-25 07:09:04.384754+00:00   \n",
       "5 2025-09-25 07:09:04.384754+00:00 2025-09-25 07:09:11.630661+00:00   \n",
       "6 2025-09-25 07:09:11.630661+00:00                              NaT   \n",
       "\n",
       "             name designation              date_started        _dlt_load_id  \\\n",
       "0  Vincent Crabbe     student 1991-09-01 09:00:00+00:00  1758783937.9341166   \n",
       "1   Gregory Goyle     student 1991-09-01 09:00:00+00:00  1758783937.9341166   \n",
       "2    Draco Malfoy     student 1991-09-01 09:00:00+00:00  1758783937.9341166   \n",
       "3    Draco Malfoy    expelled 1991-09-01 09:00:00+00:00   1758783960.934666   \n",
       "4   Maximo Malfoy    expelled 1991-09-01 09:00:00+00:00   1758784126.801204   \n",
       "5    Draco Malfoy     student 1991-09-01 09:00:00+00:00   1758784144.384754   \n",
       "6    Draco Malfoy    expelled 1991-09-01 09:00:00+00:00  1758784151.6306615   \n",
       "\n",
       "          _dlt_id  \n",
       "0  P94fUt8jUkzx6A  \n",
       "1  Wv4kISjkizZwkQ  \n",
       "2  r7u0vNojh+gvEQ  \n",
       "3  KCWs+o6jOW/aLQ  \n",
       "4  HcX8XVCthXfi5Q  \n",
       "5  r7u0vNojh+gvEQ  \n",
       "6  KCWs+o6jOW/aLQ  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run pipeline with updated dataset:\n",
    "load_info = pipeline.run(\n",
    "    data_updated,\n",
    "    table_name=\"creatures\",\n",
    "    write_disposition={\n",
    "        \"disposition\": \"merge\", # <--- specifies that existing data should be merged rather than replaced\n",
    "        \"strategy\": \"scd2\" # <--- enables SCD2 tracking, which keeps historical records of changes\n",
    "    }\n",
    ")\n",
    "print(load_info)\n",
    "\n",
    "pipeline.dataset().creatures.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcb61ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48045dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d6539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9925a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612ff66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1478c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0806d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
